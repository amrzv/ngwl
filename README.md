Конкурс https://www.kaggle.com/c/ngwl-predict-customer-churn/submissions

Были использованы таблицы `train.csv`, `addresses.csv`, `shipments/`. 
Для каждого пользователя найдены адреса с доставкой. Затем для каждого пользователя по всем посылкам (из всех его адресов) были посчитаны средние стоимость, promo, вес и рейтинг. И так отдельно для каждого месяца.

В качестве модели была использована двухслойная рекуррентная нейронная сеть LSTM(hidden_layers=128). Для каждого пользователя имеется 4 числа для каждого месяца. Месяцы с января по июль - обучение, с февраля по август - тест (для предикта). Каждый пользователь представлятся матрицей 7x4, в качестве выходов было 7 таргетов из train.csv

Обучение проводилось в режиме multi input - multi outpul. Один объект обучающей выборки это: матрица 7х4, на выходе вектор 7x1.

В качестве лосса бинарная кросс-ентропия (для каждого месяца отдельно). Поскольку нас интересовал прогноз только на последний месяц, для самбита брался только последний столбец из всей матрицы предсказаний. В частности, `f1_score`, что считался во время обучения, не совсем корректен - это лосс по всем месяцам, нас больше интересовал только последний месяц.

Это даёт: 0.70401 / 0.70073 на public/private соответственно.
